{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcJRaRbwfyXq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import os\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n"
      ],
      "metadata": {
        "id": "c6x872Bdf-50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip \"/content/drive/MyDrive/Biometrics Final Version/FaceShape Dataset.zip\""
      ],
      "metadata": {
        "id": "TTvUgotjg0lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLAP0TukfyXr"
      },
      "outputs": [],
      "source": [
        "train_ds = load_dataset(\"imagefolder\", data_dir='/content/FaceShape Dataset/training_set', split=\"train\")\n",
        "test_ds = load_dataset(\"imagefolder\", data_dir='/content/FaceShape Dataset/testing_set', split=\"train\")\n",
        "# label2idx and idx2label\n",
        "\n",
        "id2label = {id:label for id, label in enumerate(train_ds.features['label'].names)}\n",
        "label2id = {label:id for id,label in id2label.items()}\n",
        "# split train, val\n",
        "splits = test_ds.train_test_split(test_size=0.5, shuffle=True, seed=42)\n",
        "test_ds, val_ds = splits[\"train\"], splits[\"test\"]\n",
        "print(\"Features\", train_ds.features)\n",
        "print(\"Train\", train_ds)\n",
        "print(\"Validation\", val_ds)\n",
        "print(\"Test\", test_ds)\n",
        "print(\"Num labels\", len(label2id))\n",
        "print(\"Label2Idx\", label2id)\n",
        "print(\"Label2Idx\", id2label)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dva7zWNHfyXt"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt  \n",
        "from random import randint\n",
        "list_idx = [randint(0, len(train_ds)) for i in range(9)]\n",
        "def display_examples():\n",
        "    fig = plt.figure(figsize=(12,12))\n",
        "    fig.suptitle(\"Some examples of images of the dataset\", fontsize=30)\n",
        "    for i, idx in enumerate(list_idx):\n",
        "        plt.subplot(3,3,i+1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.grid(False)\n",
        "        plt.imshow(train_ds[idx][\"image\"], cmap=plt.cm.binary)\n",
        "        plt.xlabel(id2label[train_ds[idx][\"label\"]])\n",
        "    plt.show()\n",
        "\n",
        "display_examples()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8n8SPmlfyXu"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLDOLdoVfyXv"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "from transformers import AutoFeatureExtractor\n",
        "import torchvision.transforms.functional\n",
        "\n",
        "from torchvision.transforms import (\n",
        "    CenterCrop, \n",
        "    Compose, \n",
        "    Normalize, \n",
        "    RandomVerticalFlip,\n",
        "    RandomResizedCrop, \n",
        "    Resize, \n",
        "    RandomAdjustSharpness,\n",
        "    ToTensor,\n",
        "    ConvertImageDtype,\n",
        "    RandomPerspective,\n",
        "    RandomRotation,\n",
        "    \n",
        ")\n",
        "\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n",
        "\n",
        "normalize = Normalize(mean=0, std=1)\n",
        "_train_transforms = Compose(\n",
        "        [\n",
        "\n",
        "            ToTensor(),\n",
        "            Resize(list(feature_extractor.size.values())),\n",
        "            RandomVerticalFlip(p=0.05),\n",
        "            RandomPerspective(0.05,0.05),\n",
        "            RandomRotation(5),\n",
        "            # RandomAdjustSharpness(2, 0.8),\n",
        "            normalize,\n",
        "            ConvertImageDtype(torch.float)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "_val_transforms = Compose(\n",
        "        [\n",
        "            ToTensor(),\n",
        "            Resize(list(feature_extractor.size.values())),\n",
        "            normalize,\n",
        "            ConvertImageDtype(torch.float)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Biometrics Final Version/')\n",
        "from landmark_detection_new import Landmarks\n",
        "landmarks=Landmarks(\"/content/drive/MyDrive/Biometrics Final Version/shape_predictor_68_face_landmarks.dat\")\n",
        "\n",
        "def train_transforms(examples):\n",
        "    examples['pixel_values'] = [landmarks.image_mask_concat(np.asarray(image.convert(\"RGB\")),thickness=2)/255 for image in examples['image']]    \n",
        "    examples['pixel_values'] = [_train_transforms(image) for image in examples['pixel_values']]\n",
        "    return examples\n",
        "\n",
        "def val_transforms(examples):\n",
        "    examples['pixel_values'] = [landmarks.image_mask_concat(np.asarray(image.convert(\"RGB\")),thickness=2)/255 for image in examples['image']]    \n",
        "    examples['pixel_values'] = [_val_transforms(image) for image in examples['pixel_values']]\n",
        "    return examples\n",
        "\n",
        "# Set the transforms\n",
        "train_ds.set_transform(train_transforms)\n",
        "val_ds.set_transform(val_transforms)\n",
        "test_ds.set_transform(val_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vT6e960fyXy"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "\n",
        "def collate_fn(examples):\n",
        "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
        "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
        "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0IE2Qd7fyXz"
      },
      "outputs": [],
      "source": [
        "from transformers import SwinForImageClassification, SwinConfig\n",
        "\n",
        "config = SwinConfig.from_pretrained(\n",
        "        \"microsoft/swin-tiny-patch4-window7-224\",\n",
        "        num_labels=len(label2id),\n",
        "        label2id=label2id,\n",
        "        id2label=id2label,\n",
        "        finetuning_task=\"image-classification\",\n",
        "        num_channels = 4\n",
        "    )\n",
        "\n",
        "model = SwinForImageClassification.from_pretrained(\n",
        "    \"microsoft/swin-tiny-patch4-window7-224\",\n",
        "    config=config,\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "# model.load_state_dict(torch.load(\"/content/drive/MyDrive/Biometrics Project/transformers_weights_only.pt\")) #uncomment if you want to load pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBjI6Ba7fyX0"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "metric_name = \"accuracy\"\n",
        "args = TrainingArguments(\n",
        "    f\"faceshape\",\n",
        "    save_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=5e-6,\n",
        "    per_device_train_batch_size=80,\n",
        "    per_device_eval_batch_size=64,\n",
        "    num_train_epochs=15,\n",
        "    weight_decay=0.1,\n",
        "    load_best_model_at_end=False,\n",
        "    save_total_limit=1,\n",
        "    metric_for_best_model=metric_name,\n",
        "    logging_dir='logs',\n",
        "    logging_steps=1,\n",
        "    remove_unused_columns=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LE-EdcR0fyX0"
      },
      "outputs": [],
      "source": [
        "from datasets import load_metric\n",
        "import numpy as np\n",
        "\n",
        "metric = load_metric(metric_name)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFcu3tdRfyX0"
      },
      "outputs": [],
      "source": [
        "model=torch.load(\"/content/drive/MyDrive/Biometrics Final Version/transformers_final.pt\")\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    data_collator=collate_fn,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=feature_extractor,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "4QAPlNO6aDt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krDTwyzWfyX1"
      },
      "outputs": [],
      "source": [
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"WANDB_MODE\"] = \"offline\"\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5PcL9i-fyX1"
      },
      "outputs": [],
      "source": [
        "PATH = \"/content/drive/MyDrive/Biometrics Final Version/transformers_final.pt\"\n",
        "\n",
        "# Save\n",
        "# torch.save(model, PATH) #uncomment if you want to save the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nx-3gOzKfyX1"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9em_2mw9fyX1"
      },
      "outputs": [],
      "source": [
        "\n",
        "outputs = trainer.predict(test_ds)\n",
        "print(outputs.metrics)\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "y_true = outputs.label_ids\n",
        "y_pred = outputs.predictions.argmax(1)\n",
        "labels = train_ds.features['label'].names"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Classification report\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true, y_pred, target_names=labels))"
      ],
      "metadata": {
        "id": "D1iuj8npHmKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Confusion matrix\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "disp.plot()"
      ],
      "metadata": {
        "id": "kzmUqkAVHqLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EKV17cpfyX2"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from random import randint\n",
        "\n",
        "list_idx = [randint(0, len(test_ds)) for i in range(9)]\n",
        "\n",
        "def display_examples():\n",
        "    fig = plt.figure(figsize=(12,12))\n",
        "    fig.suptitle(\"Some examples of images of the test set\", fontsize=30)\n",
        "    for i,idx in enumerate(list_idx):\n",
        "        plt.subplot(3,3,i+1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.grid(False)\n",
        "        plt.imshow(test_ds[idx][\"image\"], cmap=plt.cm.binary)\n",
        "        plt.xlabel(\"Label: \"+id2label[y_true[idx]]+\"\\nPred: \"+id2label[y_pred[idx]])\n",
        "    plt.show()\n",
        "\n",
        "display_examples()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}